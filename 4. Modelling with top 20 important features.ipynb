{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find important features and use the top 20 for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "raw = pd.read_csv('OnlineNewsPopularity/OnlineNewsPopularity.csv')\n",
    "raw.columns = [c.strip() for c in raw.columns]\n",
    "raw.drop(columns=['url', 'timedelta'], inplace=True)\n",
    "raw['popular'] = raw['shares'].apply(lambda row: 0 if row < 1400 else 1)\n",
    "raw.drop(columns=['shares'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = raw.drop(columns=['popular'])\n",
    "y = raw['popular']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = ['data_channel_is_lifestyle', 'data_channel_is_bus',\n",
    "               'data_channel_is_entertainment', 'data_channel_is_socmed',\n",
    "               'data_channel_is_tech', 'data_channel_is_world', 'weekday_is_monday', \n",
    "               'weekday_is_tuesday', 'weekday_is_wednesday', 'weekday_is_thursday',\n",
    "               'weekday_is_friday', 'weekday_is_saturday', 'weekday_is_sunday', 'is_weekend']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "def sep_scale_combine(X, cat_vars):\n",
    "    X_cat = X[cat_vars]\n",
    "    X_cont = X.drop(columns=cat_vars, axis=1)\n",
    "\n",
    "    X_cont[X_cont.columns] = scaler.fit_transform(X_cont)\n",
    "    X_scaled = pd.concat([X_cat, X_cont], axis=1)\n",
    "    return X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Random Forest Model with 'improved' hyperparameters\n",
    "rf = RandomForestClassifier(n_estimators=750, max_depth=30, min_samples_leaf=5)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6636119051622666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "kw_avg_avg                       0.050863\n",
       "kw_max_avg                       0.043859\n",
       "LDA_02                           0.035849\n",
       "self_reference_min_shares        0.035110\n",
       "self_reference_avg_sharess       0.031961\n",
       "kw_avg_min                       0.029493\n",
       "LDA_01                           0.029234\n",
       "LDA_04                           0.028870\n",
       "kw_avg_max                       0.028844\n",
       "LDA_00                           0.027958\n",
       "n_unique_tokens                  0.027381\n",
       "n_non_stop_unique_tokens         0.027017\n",
       "kw_min_avg                       0.026124\n",
       "self_reference_max_shares        0.025981\n",
       "global_subjectivity              0.025600\n",
       "n_tokens_content                 0.025429\n",
       "average_token_length             0.025200\n",
       "LDA_03                           0.024998\n",
       "global_rate_positive_words       0.024735\n",
       "global_sentiment_polarity        0.024122\n",
       "kw_max_min                       0.024017\n",
       "avg_positive_polarity            0.023659\n",
       "avg_negative_polarity            0.022508\n",
       "global_rate_negative_words       0.021958\n",
       "rate_negative_words              0.020797\n",
       "num_hrefs                        0.020079\n",
       "rate_positive_words              0.019625\n",
       "kw_min_max                       0.016222\n",
       "is_weekend                       0.015541\n",
       "data_channel_is_entertainment    0.014713\n",
       "num_imgs                         0.014085\n",
       "min_negative_polarity            0.013217\n",
       "n_tokens_title                   0.012373\n",
       "title_subjectivity               0.012327\n",
       "title_sentiment_polarity         0.011297\n",
       "abs_title_subjectivity           0.011268\n",
       "max_negative_polarity            0.011149\n",
       "abs_title_sentiment_polarity     0.011010\n",
       "num_self_hrefs                   0.010631\n",
       "min_positive_polarity            0.010523\n",
       "data_channel_is_world            0.009639\n",
       "max_positive_polarity            0.009350\n",
       "num_keywords                     0.008751\n",
       "data_channel_is_tech             0.008319\n",
       "kw_max_max                       0.007525\n",
       "data_channel_is_socmed           0.006902\n",
       "kw_min_min                       0.006676\n",
       "num_videos                       0.006080\n",
       "weekday_is_saturday              0.004721\n",
       "weekday_is_sunday                0.002839\n",
       "weekday_is_wednesday             0.002309\n",
       "weekday_is_tuesday               0.002243\n",
       "data_channel_is_bus              0.002192\n",
       "weekday_is_thursday              0.002031\n",
       "weekday_is_monday                0.001850\n",
       "weekday_is_friday                0.001729\n",
       "data_channel_is_lifestyle        0.001139\n",
       "n_non_stop_words                 0.000077\n",
       "dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n",
    "\n",
    "important_features = pd.Series(data=rf.feature_importances_,index=X.columns)\n",
    "important_features.sort_values(ascending=False,inplace=True)\n",
    "important_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = important_features.index[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.651589036488986\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "rf = RandomForestClassifier(n_estimators=750, max_depth=30, min_samples_leaf=5)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "print('Accuracy: ', accuracy_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min = 0\n",
    "index_max = 10000\n",
    "rf = RandomForestClassifier(n_estimators=750, max_depth=20, min_samples_leaf=10)\n",
    "rf_predictions = pd.DataFrame(columns=['y_actual', 'y_predict'], dtype=int)\n",
    "acc_training = []\n",
    "\n",
    "while index_max <= 39000:\n",
    "    X_train_rf = X.iloc[index_min : index_max]\n",
    "    y_train_rf = y.iloc[index_min : index_max]\n",
    "    \n",
    "    X_test_rf = X.iloc[index_max : (index_max + 1000)]\n",
    "    y_test_rf = y.iloc[index_max : (index_max + 1000)]\n",
    "    \n",
    "    rf.fit(X_train_rf, y_train_rf)\n",
    "    acc_training.append(rf.score(X_train_rf, y_train_rf))\n",
    "    y_predictions_rf = rf.predict(X_test_rf)\n",
    "    y_actual_and_predict_rf = pd.DataFrame({'y_actual': y_test_rf, 'y_predict': y_predictions_rf})\n",
    "    rf_predictions = rf_predictions.append(y_actual_and_predict_rf, ignore_index=False)\n",
    "    index_min += 1000\n",
    "    index_max += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8757, 0.8765, 0.8742, 0.8757, 0.8762, 0.8731, 0.8713, 0.8629, 0.8564, 0.8576, 0.8586, 0.8631, 0.8635, 0.8618, 0.8618, 0.8569, 0.8547, 0.8526, 0.8519, 0.8506, 0.8506, 0.8445, 0.8475, 0.8492, 0.8465, 0.8443, 0.8435, 0.8415, 0.843, 0.8447]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6544326001889084"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(acc_training)\n",
    "accuracy_score(rf_predictions['y_actual'].values, rf_predictions['y_predict'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'min_samples_leaf': 10, 'n_estimators': 750}\n",
      "0.6491508323524466\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf_params = {'n_estimators': [200, 400, 750, 1000], 'max_depth' : [15, 20, 30], 'min_samples_leaf' : [1, 5, 10]}\n",
    "gsRf = GridSearchCV(estimator=rf, param_grid=rf_params)\n",
    "\n",
    "gsRf.fit(X_train, y_train)\n",
    "print(gsRf.best_params_ )\n",
    "gsRf.predict(X_test)\n",
    "print(gsRf.score(X_test, y_test)) # Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat window with scaled data and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 20, 'min_samples_leaf': 5, 'n_estimators': 1000}\n",
      "0.6615100050445603\n"
     ]
    }
   ],
   "source": [
    "# Scale and optimize Random Forest hyperparameters. \n",
    "X_scaled = X.copy()\n",
    "X_scaled[X_scaled.columns] = scaler.fit_transform(X_scaled)\n",
    "X_train_scaled, X_test_scaled, y_train_scaled, y_test_scaled = train_test_split(X_scaled, y, test_size=0.3, random_state=15)\n",
    "\n",
    "rf_scaled = RandomForestClassifier()\n",
    "rf_params_scaled = {'n_estimators': [200, 400, 750, 1000], 'max_depth' : [15, 20, 30], 'min_samples_leaf' : [1, 5, 10]}\n",
    "gsRf_scaled = GridSearchCV(estimator=rf_scaled, param_grid=rf_params_scaled)\n",
    "\n",
    "gsRf_scaled.fit(X_train_scaled, y_train_scaled)\n",
    "print(gsRf_scaled.best_params_ )\n",
    "gsRf_scaled.predict(X_test_scaled)\n",
    "print(gsRf_scaled.score(X_test_scaled, y_test_scaled)) # Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_min = 0\n",
    "index_max = 10000\n",
    "rf_scaled = RandomForestClassifier(n_estimators=1000, max_depth=20, min_samples_leaf=5)\n",
    "rf_predictions_scaled = pd.DataFrame(columns=['y_actual', 'y_predict'], dtype=int)\n",
    "acc_training_scaled = []\n",
    "\n",
    "while index_max <= 39000:\n",
    "    X_train_rf = X.iloc[index_min : index_max]\n",
    "    X_train_rf_scaled = scaler.fit_transform(X_train_rf)\n",
    "    y_train_rf = y.iloc[index_min : index_max]\n",
    "    \n",
    "    X_test_rf = X.iloc[index_max : (index_max + 1000)]\n",
    "    X_test_rf_scaled = scaler.fit_transform(X_test_rf)\n",
    "    y_test_rf = y.iloc[index_max : (index_max + 1000)]\n",
    "    \n",
    "    rf_scaled.fit(X_train_rf_scaled, y_train_rf)\n",
    "    acc_training_scaled.append(rf.score(X_train_rf_scaled, y_train_rf))\n",
    "    y_predictions_rf = rf_scaled.predict(X_test_rf_scaled)\n",
    "    y_actual_and_predict_rf = pd.DataFrame({'y_actual': y_test_rf, 'y_predict': y_predictions_rf})\n",
    "    rf_predictions_scaled = rf_predictions_scaled.append(y_actual_and_predict_rf, ignore_index=False)\n",
    "    index_min += 1000\n",
    "    index_max += 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4612, 0.4676, 0.4787, 0.4846, 0.4992, 0.5136, 0.5266, 0.5303, 0.5281, 0.5175, 0.5072, 0.5006, 0.4875, 0.4853, 0.4769, 0.4715, 0.4666, 0.4693, 0.4709, 0.4814, 0.4815, 0.4822, 0.5008, 0.5067, 0.5141, 0.5215, 0.5329, 0.536, 0.5367, 0.5397]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6408042099581703"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(acc_training_scaled)\n",
    "accuracy_score(rf_predictions_scaled['y_actual'].values, rf_predictions_scaled['y_predict'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the training set accuracy, even when changed significantly (by 0.5), the testing set acccuracy is consistent. Small difference of .02 can be accounted for by randomness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
